{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_6-jdBOXowG"
      },
      "source": [
        "**注意事項**\n",
        "\n",
        "このノートブックは、GPU:「T4」に対応させたものです。\n",
        "「L4」版のノートブックとはモデル等が異なるため、生成される内容が異なることが考えられます。\n",
        "\n",
        "生成される内容と、ノートブックに記載されている説明が一致しない場合があることをご了承ください。\n",
        "\n",
        "生成内容とノートブックの説明をよく見比べ、適宜読み替えながら演習を進めてみてください。\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2vGvZiRGLtu"
      },
      "source": [
        "# 演習の方針\n",
        "\n",
        "1. **ベースラインモデル評価**  \n",
        "   素のモデルで回答を生成し、講義内容との整合性の低さを観察します。これにより、特別な学習なしでのモデルの限界を確認します。\n",
        "\n",
        "2. **文字起こしデータの活用**  \n",
        "   講義の文字起こしデータを導入し、モデルが講義内容を参照した回答を生成する傾向を観察します。ただし、Retrieval（情報検索）精度の限界から結果は不安定になる可能性があります。\n",
        "\n",
        "3. **チャンク化の導入**  \n",
        "   文字起こしデータをチャンク（小単位）に分割し、より安定して関連コンテンツを取得できるようにします。この段階では文脈理解にまだ課題があることを確認します。\n",
        "\n",
        "4. **Rerankの適用**  \n",
        "   検索結果のランク付けを導入し、より的確で安定した回答を目指します。\n",
        "\n",
        "5. **応用改善手法**  \n",
        "   文字起こしの品質向上のための編集技術や、メタデータの活用による性能向上手法を探ります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPI1pj4mFavt"
      },
      "source": [
        "## 扱う質問\n",
        "\n",
        "「Inference Time Scaling（推論時スケーリング）」に関する質問を取り扱います。これは以下の背景を持つトピックです。\n",
        "\n",
        "- 2024年8月発表の論文「Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters」で提唱された概念\n",
        "- OpenAIのGPT-o1（2024年9月リリース）で実用化され、注目を集めた比較的新しいアプローチ\n",
        "- 2024年度LLM講座の第4回講義でも取り上げられた重要テーマ\n",
        "\n",
        "## 扱うモデル\n",
        "\n",
        "「google/gemma-2-2b-jpn-it」を使用します。このモデルは、リリース時期の関係上、以下の特徴を持ちます。\n",
        "\n",
        "- 「Inference Time Scaling」の概念が広まる前に訓練されており、このトピックに関する知識を持たないと想定される\n",
        "- この特性を活かし、純粋なベースライン評価から各手法の効果を観察する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfAcas6WGLtu"
      },
      "source": [
        "### 演習環境の準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vM50WAI7GXwC"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers\n",
        "!pip install google-colab-selenium\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2PStE0uqM03"
      },
      "outputs": [],
      "source": [
        "# 演習用のコンテンツを取得\n",
        "!git clone https://github.com/tshigata/lecture-ai-engineering.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXo_kFASXlvp"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "# HuggingFace Login\n",
        "from huggingface_hub import notebook_login\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(token=hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZ_NUIftXwLc"
      },
      "outputs": [],
      "source": [
        "# CUDAが利用可能ならGPUを、それ以外ならCPUをデバイスとして設定\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eTgV8XBPA90"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tV9mO8oXoaM"
      },
      "outputs": [],
      "source": [
        "# モデル(Gemma2)の読み込み\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "model_name = \"google/gemma-2-2b-jpn-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=\"auto\",\n",
        "            quantization_config=bnb_config,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 質問リストを定義\n",
        "questions = [\n",
        "    \"松尾・岩澤研究室の講座において、退会した後も研究室が受講者の情報を利用できる条件とは何ですか？\",\n",
        "    \"未成年者が講座を受講するには、どのような条件が求められますか？\",\n",
        "    \"受講者が他の人のSlack投稿をSNSで共有することは、講座規約上どう扱われますか？\",\n",
        "    \"講座で使用された資料や動画を、受講後に再利用・再配布することは許可されていますか？\",\n",
        "    \"講義中にノイズなどのトラブルを発生させた場合、講座への参加はどうなりますか？\"\n",
        "]"
      ],
      "metadata": {
        "id": "A55HtXg82dhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for question in questions:\n",
        "  print(question)\n"
      ],
      "metadata": {
        "id": "mp9zcMnV-Nmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vTIPvudryW_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piTdVxTfGcc_"
      },
      "source": [
        "# 1. ベースラインモデル評価\n",
        "**まずはベースモデルがどの程度知識を持っているか確かめる**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def llm_answer(question):\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": question},\n",
        "    ]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    terminators = [\n",
        "        tokenizer.eos_token_id,\n",
        "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "    ]\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=256,\n",
        "        eos_token_id=terminators,\n",
        "        do_sample=False,\n",
        "    )\n",
        "\n",
        "    response = outputs[0][input_ids.shape[-1]:]\n",
        "\n",
        "    return tokenizer.decode(response, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "qYeDDIBz2c8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ベースライン回答の生成（RAGなし）\n",
        "baseline_answers = [llm_answer(q) for q in questions]\n",
        "for answer in baseline_answers:\n",
        "  print(answer)"
      ],
      "metadata": {
        "id": "mYe7AK2v2nqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "emb_model = SentenceTransformer(\"infly/inf-retriever-v1-1.5b\", trust_remote_code=True)\n",
        "# In case you want to reduce the maximum length:\n",
        "emb_model.max_seq_length = 4096"
      ],
      "metadata": {
        "id": "Qhp7oQOQ2z7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. RAGモデル評価"
      ],
      "metadata": {
        "id": "boHqjEtJsbCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/lecture-ai-engineering/day3/data/松尾・岩澤研究室 講座受講規約.txt\", \"r\", encoding=\"shift_jis\") as f:\n",
        "    raw_writedown = f.read()"
      ],
      "metadata": {
        "id": "yZF0Ro8C2z3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ドキュメントを用意する。\n",
        "documents = [text.strip() for text in raw_writedown.split(\"。\")]\n",
        "print(\"ドキュメントサイズ: \", len(documents))\n",
        "print(\"ドキュメントの例: \\n\", documents[0:2])"
      ],
      "metadata": {
        "id": "mO8gcFtw2z0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# GPUのメモリを解放\n",
        "torch.cuda.empty_cache() #-> NG\n",
        "\n",
        "# # CPUに切り替え\n",
        "emb_model = emb_model.to(\"cpu\")"
      ],
      "metadata": {
        "id": "gC0VtPdwB1xD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 参照テキストのembeddingを生成する\n",
        "doc_embeddings = emb_model.encode(documents, convert_to_tensor=True)"
      ],
      "metadata": {
        "id": "9yqcqzrsBh9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_references_from_question(question, topk=5):\n",
        "    q_embedding = emb_model.encode([question], convert_to_tensor=True)\n",
        "    scores = torch.matmul(q_embedding, doc_embeddings.T)[0].cpu().numpy()\n",
        "    top_indices = scores.argsort()[::-1][:topk]\n",
        "    return \"\\n\".join([f\"* {documents[i]}\" for i in top_indices])"
      ],
      "metadata": {
        "id": "_5_8gVu9BgxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_prompts = [\n",
        "    f\"以下の講座規約を参考にして質問に答えてください。\\n\\n{get_references_from_question(q)}\\n\\n質問：{q}\"\n",
        "    for q in questions\n",
        "]\n",
        "\n",
        "print(rag_prompts)"
      ],
      "metadata": {
        "id": "9k_EXu2bBgut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_answers = []\n",
        "for i, q in enumerate(questions):\n",
        "  rag_prompt = f\"以下の講座規約を参考にして質問に答えてください。\\n\\n{get_references_from_question(q)}\\n\\n質問：{q}\"\n",
        "  rag_response = llm_answer(rag_prompt)\n",
        "  rag_answers.append(rag_response)"
      ],
      "metadata": {
        "id": "ntgmXvC2BgsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: # questions, baseline_answers, rag_answers を一つのDataframeに収める\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming questions, baseline_answers, and rag_answers are already defined as in your provided code\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'questions': questions,\n",
        "    'baseline_answers': baseline_answers,\n",
        "    'rag_answers': rag_answers\n",
        "})\n",
        "\n",
        "df\n"
      ],
      "metadata": {
        "id": "J-FUzWXBLYu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"llm_responses.csv\", index=False, encoding=\"utf-8\")"
      ],
      "metadata": {
        "id": "qu3Q5ik0DliV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_answers"
      ],
      "metadata": {
        "id": "KFFLUaszDlfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: questions、baseline_answers、rag_answersを一つのテーブルに変換して、CSVファイルを作成\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming questions, baseline_answers, and rag_answers are already defined from the previous code\n",
        "\n",
        "results = []\n",
        "for i in range(len(questions)):\n",
        "    results.append({\n",
        "        \"question\": questions[i],\n",
        "        \"baseline_answer\": baseline_answers[i],\n",
        "        \"rag_answer\": rag_answers[i]\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv(\"llm_responses.csv\", index=False, encoding=\"utf-8\")\n"
      ],
      "metadata": {
        "id": "QgSK4-KdDldT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. LLM as a Judge"
      ],
      "metadata": {
        "id": "oaIQGtuesStf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "golden_answers = [\n",
        "    # Q1: 退会後も研究室が受講者の情報を利用できる条件\n",
        "    \"受講者が退会した後であっても、在籍中に取得された情報はプライバシーポリシーを遵守する形で、本規約に従って研究室が引き続き利用することができます。\",  #:contentReference[oaicite:0]{index=0}\n",
        "\n",
        "    # Q2: 未成年者が講座を受講する条件\n",
        "    \"未成年者が受講を希望する場合は、事前に法定代理人の同意を得ている必要があります。\",  #:contentReference[oaicite:1]{index=1}\n",
        "\n",
        "    # Q3: 他人のSlack投稿をSNSで共有することの取り扱い\n",
        "    \"Slack上でのやりとりやスクリーンショットをSNSにアップすることは禁止されています。\",  #:contentReference[oaicite:2]{index=2}\n",
        "\n",
        "    # Q4: 資料や動画の再利用・再配布の可否\n",
        "    \"講義資料や動画のURLについて、受講後であっても無断での再利用や配布は認められていません。\",  #:contentReference[oaicite:3]{index=3}\n",
        "\n",
        "    # Q5: ノイズなどのトラブル時の対応\n",
        "    \"受講者のインフラに起因するノイズ等のトラブルで他の受講環境に悪影響がある場合、改善するまで参加を一時的に制限されることがあります。\"  #:contentReference[oaicite:4]{index=4}\n",
        "]"
      ],
      "metadata": {
        "id": "3lcFeDRfsSXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HTML読み込み（テキストとして評価に含める）\n",
        "with open(\"/content/lecture-ai-engineering/day3/data/松尾・岩澤研究室 講座受講規約.html\", \"r\", encoding=\"utf-8\") as f:\n",
        "    reference_html = f.read()"
      ],
      "metadata": {
        "id": "H2T9B5Y6NuKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template_three_criteria_with_ref = (\n",
        "    \"You are an expert evaluator for answers generated by an AI system.\\n\"\n",
        "    \"Below is the full official reference document that should be used to evaluate the answers.\\n\\n\"\n",
        "    \"[Reference Document Start]\\n{reference_text}\\n[Reference Document End]\\n\\n\"\n",
        "    \"Please evaluate the User Answer against this reference document and the question provided using the following criteria:\\n\"\n",
        "    \"1. Correctness (0-5): Is the User Answer factually correct based on the reference document?\\n\"\n",
        "    \"2. Completeness (0-5): Does the User Answer cover all necessary points from the reference document?\\n\"\n",
        "    \"3. Relevance (0-5): Is the User Answer directly relevant to the question?\\n\\n\"\n",
        "    \"Only return your answer in this format:\\n\"\n",
        "    \"Correctness: <0-5>\\nCompleteness: <0-5>\\nRelevance: <0-5>\\n\\n\"\n",
        "    \"### Question:\\n{query}\\n\\n### User Answer:\\n{user_answer}\"\n",
        ")"
      ],
      "metadata": {
        "id": "J_zWNM2QN08I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template_strict_eval = (\n",
        "    \"You are an expert evaluator judging whether a User Answer is factually aligned with a given official document.\\n\\n\"\n",
        "    \"The question has only one correct answer, and it must be consistent with the provided reference document.\\n\"\n",
        "    \"Please strictly evaluate the User Answer according to the three criteria below, using ONLY the reference document.\\n\\n\"\n",
        "    \"If a fact or claim is not explicitly supported by the reference document, deduct points.\\n\"\n",
        "    \"Even if the User Answer sounds fluent or plausible, deduct points if it includes hallucinated or irrelevant content.\\n\\n\"\n",
        "    \"Criteria:\\n\"\n",
        "    \"1. Correctness (0-5): Factually correct and consistent with the reference?\\n\"\n",
        "    \"2. Completeness (0-5): Does it fully answer the question based on the reference?\\n\"\n",
        "    \"3. Relevance (0-5): Does it directly address the question without extraneous or fabricated content?\\n\\n\"\n",
        "    \"Answer format (numbers only):\\n\"\n",
        "    \"Correctness: <0-5>\\nCompleteness: <0-5>\\nRelevance: <0-5>\\n\\n\"\n",
        "    \"### Reference Document:\\n{reference_text}\\n\\n\"\n",
        "    \"### Question:\\n{query}\\n\\n\"\n",
        "    \"### User Answer:\\n{user_answer}\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "gMj-IwT5POw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_answer_with_reference(query, user_answer, golden_answer):\n",
        "    \"\"\"\n",
        "    golden_answer（模範解答）を参照して user_answer の正確性・完全性・関連性を評価します。\n",
        "    OpenAI GPT-4o-miniを使って3軸評価（0-5）を返します。\n",
        "    \"\"\"\n",
        "\n",
        "    # 厳格な評価テンプレート（golden_answerをreferenceとして使う）\n",
        "    prompt = (\n",
        "        \"あなたはAIによる回答の評価者です。\\n\\n\"\n",
        "        \"以下は「ある質問」に対するユーザーの回答と、それに対応する模範解答（golden answer）です。\\n\"\n",
        "        \"模範解答と比較して、ユーザーの回答がどの程度正確で、完全で、質問に直接関連しているかを評価してください。\\n\\n\"\n",
        "        \"評価基準は以下の通りです：\\n\"\n",
        "        \"1. 正確性（Correctness）: golden answerと矛盾なく事実に基づいているか？\\n\"\n",
        "        \"2. 完全性（Completeness）: golden answerに含まれる要点をどれだけカバーしているか？\\n\"\n",
        "        \"3. 関連性（Relevance）: 回答が質問に対して直接的か？無関係な話が混じっていないか？\\n\\n\"\n",
        "        \"それぞれ0〜5点で評価し、以下の形式で返してください。\\n\\n\"\n",
        "        \"Correctness: <0-5>\\nCompleteness: <0-5>\\nRelevance: <0-5>\\n\\n\"\n",
        "        \"### 質問:\\n{query}\\n\\n\"\n",
        "        \"### ユーザーの回答:\\n{user_answer}\\n\\n\"\n",
        "        \"### 模範解答:\\n{golden_answer}\\n\"\n",
        "    ).format(query=query, user_answer=user_answer, golden_answer=golden_answer)\n",
        "\n",
        "    try:\n",
        "        response = openai_generator(prompt)\n",
        "        scores = [int(s) for s in re.findall(r\"\\d+\", response)]\n",
        "\n",
        "        if len(scores) == 3:\n",
        "            return scores\n",
        "        else:\n",
        "            print(\"スコア抽出失敗:\", response)\n",
        "            return [0, 0, 0]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"評価エラー:\", e)\n",
        "        return [0, 0, 0]"
      ],
      "metadata": {
        "id": "lK6zi2wa1v45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "import pandas as pd\n",
        "\n",
        "client = OpenAI(api_key=userdata.get(\"OPENAI_API_KEY\"), max_retries=5, timeout=60)\n",
        "\n",
        "# OpenAI 呼び出し\n",
        "def openai_generator(prompt):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "# 評価関数：各項目ごとにスコア抽出\n",
        "import re\n",
        "\n",
        "# def evaluate_answer_with_reference(query, user_answer, reference_text):\n",
        "#     prompt = template_three_criteria_with_ref.format(\n",
        "#         query=query,\n",
        "#         user_answer=user_answer,\n",
        "#         reference_text=reference_text\n",
        "#     )\n",
        "#     output = openai_generator(prompt)\n",
        "\n",
        "#     import re\n",
        "#     try:\n",
        "#         scores = [int(s) for s in re.findall(r\"\\d+\", output)]\n",
        "#         if len(scores) == 3:\n",
        "#             return scores\n",
        "#         else:\n",
        "#             print(\"スコア抽出失敗:\", output)\n",
        "#             return [0, 0, 0]\n",
        "#     except:\n",
        "#         print(\"評価エラー:\", output)\n",
        "#         return [0, 0, 0]\n",
        "\n",
        "# 評価実行\n",
        "records = []\n",
        "\n",
        "for i in range(len(questions)):\n",
        "    row = {\n",
        "        \"question\": questions[i],\n",
        "        \"baseline_answer\": baseline_answers[i],\n",
        "        \"rag_answer\": rag_answers[i]\n",
        "    }\n",
        "\n",
        "    # golden_answers[i] を参照にして採点\n",
        "    row[\"baseline_correctness\"], row[\"baseline_completeness\"], row[\"baseline_relevance\"] = evaluate_answer_with_reference(\n",
        "        questions[i], baseline_answers[i], golden_answers[i]\n",
        "    )\n",
        "    row[\"rag_correctness\"], row[\"rag_completeness\"], row[\"rag_relevance\"] = evaluate_answer_with_reference(\n",
        "        questions[i], rag_answers[i], golden_answers[i]\n",
        "    )\n",
        "\n",
        "    records.append(row)\n",
        "\n",
        "# DataFrameに変換・保存\n",
        "df_eval = pd.DataFrame(records)\n",
        "df_eval.to_csv(\"llm_evaluation_detailed.csv\", index=False, encoding=\"utf-8\")\n",
        "df_eval\n"
      ],
      "metadata": {
        "id": "0sbRtpH8DlaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7CEBvynG3Vqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_answer_with_reference(query, user_answer, golden_answer):\n",
        "    \"\"\"\n",
        "    golden_answerを基準に user_answer を評価し、各項目のスコアとその理由（説明）を返す。\n",
        "    戻り値: (correctness, completeness, relevance, explanation)\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = (\n",
        "        \"あなたはAIによる回答の評価者です。\\n\\n\"\n",
        "        \"以下はある質問に対するユーザーの回答と、その模範解答（golden answer）です。\\n\"\n",
        "        \"以下の3つの観点でユーザー回答を0〜5点で評価し、それぞれについて簡単な理由を説明してください。\\n\\n\"\n",
        "        \"【観点】\\n\"\n",
        "        \"1. 正確性（Correctness）: golden answerと矛盾せず、事実として正しいか？\\n\"\n",
        "        \"2. 完全性（Completeness）: golden answerに含まれる要点がカバーされているか？\\n\"\n",
        "        \"3. 関連性（Relevance）: 回答が質問に直接関連し、無関係な内容を含んでいないか？\\n\\n\"\n",
        "        \"【出力形式】（必ず以下の形式で）\\n\"\n",
        "        \"Correctness: <スコア>（理由）\\n\"\n",
        "        \"Completeness: <スコア>（理由）\\n\"\n",
        "        \"Relevance: <スコア>（理由）\\n\\n\"\n",
        "        \"### 質問:\\n{query}\\n\\n\"\n",
        "        \"### ユーザーの回答:\\n{user_answer}\\n\\n\"\n",
        "        \"### 模範解答:\\n{golden_answer}\\n\"\n",
        "    ).format(query=query, user_answer=user_answer, golden_answer=golden_answer)\n",
        "\n",
        "    try:\n",
        "        response = openai_generator(prompt)\n",
        "\n",
        "        # スコアの抽出\n",
        "        scores = [int(s) for s in re.findall(r\"(?<=: )\\d\", response)]\n",
        "        explanations = {}\n",
        "        for line in response.strip().split(\"\\n\"):\n",
        "            if line.startswith(\"Correctness\"):\n",
        "                explanations[\"correctness_reason\"] = line\n",
        "            elif line.startswith(\"Completeness\"):\n",
        "                explanations[\"completeness_reason\"] = line\n",
        "            elif line.startswith(\"Relevance\"):\n",
        "                explanations[\"relevance_reason\"] = line\n",
        "\n",
        "        if len(scores) == 3:\n",
        "            return (\n",
        "                scores[0], scores[1], scores[2],\n",
        "                explanations.get(\"correctness_reason\", \"\"),\n",
        "                explanations.get(\"completeness_reason\", \"\"),\n",
        "                explanations.get(\"relevance_reason\", \"\")\n",
        "            )\n",
        "        else:\n",
        "            print(\"スコア抽出失敗:\", response)\n",
        "            return 0, 0, 0, \"N/A\", \"N/A\", \"N/A\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"評価エラー:\", e)\n",
        "        return 0, 0, 0, \"N/A\", \"N/A\", \"N/A\"\n"
      ],
      "metadata": {
        "id": "R1hyczpw3Vnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "records = []\n",
        "\n",
        "for i in range(len(questions)):\n",
        "    row = {\n",
        "        \"question\": questions[i],\n",
        "        \"baseline_answer\": baseline_answers[i],\n",
        "        \"rag_answer\": rag_answers[i],\n",
        "        \"golden_answer\": golden_answers[i]\n",
        "    }\n",
        "\n",
        "    # baseline\n",
        "    b_corr, b_comp, b_rel, b_exp_corr, b_exp_comp, b_exp_rel = evaluate_answer_with_reference(\n",
        "        questions[i], baseline_answers[i], golden_answers[i]\n",
        "    )\n",
        "    row[\"baseline_correctness\"] = b_corr\n",
        "    row[\"baseline_completeness\"] = b_comp\n",
        "    row[\"baseline_relevance\"] = b_rel\n",
        "    row[\"baseline_correctness_explanation\"] = b_exp_corr\n",
        "    row[\"baseline_completeness_explanation\"] = b_exp_comp\n",
        "    row[\"baseline_relevance_explanation\"] = b_exp_rel\n",
        "\n",
        "    # rag\n",
        "    r_corr, r_comp, r_rel, r_exp_corr, r_exp_comp, r_exp_rel = evaluate_answer_with_reference(\n",
        "        questions[i], rag_answers[i], golden_answers[i]\n",
        "    )\n",
        "    row[\"rag_correctness\"] = r_corr\n",
        "    row[\"rag_completeness\"] = r_comp\n",
        "    row[\"rag_relevance\"] = r_rel\n",
        "    row[\"rag_correctness_explanation\"] = r_exp_corr\n",
        "    row[\"rag_completeness_explanation\"] = r_exp_comp\n",
        "    row[\"rag_relevance_explanation\"] = r_exp_rel\n",
        "\n",
        "    records.append(row)\n"
      ],
      "metadata": {
        "id": "XNKiiwSbOiP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrame に変換\n",
        "df_eval = pd.DataFrame(records)"
      ],
      "metadata": {
        "id": "iw8DryBs-_c5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrame に変換\n",
        "df_eval = pd.DataFrame(records)"
      ],
      "metadata": {
        "id": "lx7onTkd_gNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_columns = [\n",
        "    \"baseline_correctness\", \"baseline_completeness\", \"baseline_relevance\",\n",
        "    \"rag_correctness\", \"rag_completeness\", \"rag_relevance\"\n",
        "]\n",
        "\n",
        "# スコア列のみ抽出して表示\n",
        "df_eval[score_columns]"
      ],
      "metadata": {
        "id": "hF7bTdBY4qi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# スコア列（数値）のみ削除\n",
        "score_columns = [\n",
        "    \"baseline_correctness\", \"baseline_completeness\", \"baseline_relevance\",\n",
        "    \"rag_correctness\", \"rag_completeness\", \"rag_relevance\"\n",
        "]\n",
        "df_eval = df_eval.drop(columns=score_columns, errors=\"ignore\")\n",
        "\n",
        "# CSV に保存\n",
        "df_eval.to_csv(\"llm_evaluation_detailed.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "df_eval"
      ],
      "metadata": {
        "id": "a8P8jewR3lol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. RAGのデバッグ"
      ],
      "metadata": {
        "id": "92O56AzaBMBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def debug_rag_chunks_for_question(index, questions, documents, doc_embeddings, emb_model, top_k=5):\n",
        "    \"\"\"\n",
        "    質問に対して、類似度の高いチャンクとそのスコアを表示する（FAISS非使用）。\n",
        "\n",
        "    Parameters:\n",
        "        index (int): questionsのインデックス\n",
        "        questions (list of str): 質問リスト\n",
        "        documents (list of str): 事前に分割された文書チャンク群\n",
        "        doc_embeddings (Tensor): documentsに対応する埋め込みベクトル（convert_to_tensor=Trueで生成）\n",
        "        emb_model: SentenceTransformerなどのエンコーダー\n",
        "        top_k (int): 表示する上位チャンク数\n",
        "    \"\"\"\n",
        "    question = questions[index]\n",
        "    print(f\"\\n=== [質問 {index}] ===\\n{question}\\n\")\n",
        "\n",
        "    # 質問の埋め込みベクトルを生成\n",
        "    q_embedding = emb_model.encode([question], convert_to_tensor=True)\n",
        "\n",
        "    # 類似度スコア（内積）を計算\n",
        "    scores = torch.matmul(q_embedding, doc_embeddings.T)[0].cpu().numpy()\n",
        "\n",
        "    # 上位 top_k のインデックスとスコア\n",
        "    top_indices = scores.argsort()[::-1][:top_k]\n",
        "\n",
        "    print(f\"--- 類似チャンク（Top {top_k}） ---\")\n",
        "    for rank, i in enumerate(top_indices):\n",
        "        print(f\"[{rank+1}] 類似度スコア: {scores[i]:.4f}\")\n",
        "        print(f\"内容: {documents[i][:300]}...\")\n",
        "        print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "S6rn8kNV3lmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "debug_rag_chunks_for_question(\n",
        "    index=2,  # questions[2]\n",
        "    questions=questions,\n",
        "    documents=documents,\n",
        "    doc_embeddings=doc_embeddings,\n",
        "    emb_model=emb_model,\n",
        "    top_k=5\n",
        ")"
      ],
      "metadata": {
        "id": "dL2UkrHD3ljY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "g7aWXsuF3lfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-67FJrCN3lZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "heo40tVu3lWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_factual_errors(query, user_answer, reference_text):\n",
        "    prompt = (\n",
        "        \"You are a strict factual checker. Given a reference document, a question, and a user answer, \"\n",
        "        \"identify any factual inaccuracies in the user answer based on the reference.\\n\\n\"\n",
        "        \"If the user answer contains claims not supported by the reference, or contradicts it, list them.（日本語で）\\n\"\n",
        "        \"If there are no factual errors, simply say: None.\\n\\n\"\n",
        "        \"### Reference Document:\\n{reference_text}\\n\\n\"\n",
        "        \"### Question:\\n{query}\\n\\n\"\n",
        "        \"### User Answer:\\n{user_answer}\\n\\n\"\n",
        "        \"Factual Errors:\"\n",
        "    ).format(reference_text=reference_text, query=query, user_answer=user_answer)\n",
        "\n",
        "    return openai_generator(prompt)\n"
      ],
      "metadata": {
        "id": "IDOsK3VkOiNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_baseline_vs_rag(query, baseline_answer, rag_answer, reference_text):\n",
        "    prompt = (\n",
        "        \"You are an impartial evaluator. Based ONLY on the reference document, determine which answer is more accurate and appropriate for the given question.\\n\\n\"\n",
        "        \"Choose the better one strictly based on factual correctness, completeness, and relevance to the question.\\n\"\n",
        "        \"If both answers are equally good, say: Equal\\n\"\n",
        "        \"Choose from: Baseline / RAG / Equal\\n\\n\"\n",
        "        \"### Reference Document:\\n{reference_text}\\n\\n\"\n",
        "        \"### Question:\\n{query}\\n\\n\"\n",
        "        \"### Baseline Answer:\\n{baseline_answer}\\n\\n\"\n",
        "        \"### RAG Answer:\\n{rag_answer}\\n\\n\"\n",
        "        \"Better Answer:\".format(\n",
        "            reference_text=reference_text,\n",
        "            query=query,\n",
        "            baseline_answer=baseline_answer,\n",
        "            rag_answer=rag_answer\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return openai_generator(prompt)\n"
      ],
      "metadata": {
        "id": "ijGdA97AOiK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "records = []\n",
        "\n",
        "for i in range(len(questions)):\n",
        "    question = questions[i]\n",
        "    base_ans = baseline_answers[i]\n",
        "    rag_ans = rag_answers[i]\n",
        "\n",
        "    # ベースラインとRAGの事実誤認指摘\n",
        "    baseline_errors = identify_factual_errors(question, base_ans, reference_html)\n",
        "    rag_errors = identify_factual_errors(question, rag_ans, reference_html)\n",
        "\n",
        "    # 相対評価\n",
        "    comparison_result = compare_baseline_vs_rag(question, base_ans, rag_ans, reference_html)\n",
        "\n",
        "    records.append({\n",
        "        \"question\": question,\n",
        "        \"baseline_answer\": base_ans,\n",
        "        \"rag_answer\": rag_ans,\n",
        "        \"baseline_errors\": baseline_errors,\n",
        "        \"rag_errors\": rag_errors,\n",
        "        \"better_answer\": comparison_result.strip()\n",
        "    })\n",
        "\n",
        "df_compare = pd.DataFrame(records)\n",
        "df_compare.to_csv(\"rag_vs_baseline_comparison.csv\", index=False, encoding=\"utf-8\")\n",
        "df_compare\n"
      ],
      "metadata": {
        "id": "1tEMmjkdOiIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nz--_N8dOiFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iqvq8qIpOiCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5BRcs4rDOh_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U6JaukycBgnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJVL3u6lCc8k"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}